{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4607ad8a",
   "metadata": {},
   "source": [
    "# Federated Learning + XAI for Parkinson's Detection — Full Notebook (v2)\n",
    "\n",
    "This **full** notebook includes:\n",
    "- **Gait classification (PD vs Control)** on *PhysioNet \"Gait in Parkinson’s Disease\"* with robust evaluation (no one-class AUC warnings) and **balanced accuracy**.\n",
    "- **Voice severity regression (UPDRS)** on *UCI Parkinson’s Telemonitoring*.\n",
    "- **Federated Learning (FedAvg)** with **subject-as-client** simulation.\n",
    "- **XAI**: **Integrated Gradients** for gait, **SHAP** for voice.\n",
    "- Safer label inference for gait files; skips non-signal files (`format.txt`, `demographics.txt`, hashes).  \n",
    "\n",
    "> Adjust the paths below to your local folders (defaults reflect your latest message).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f09d9",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcae6787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muham\\anaconda3\\envs\\parkinsons_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If needed, install packages (uncomment):\n",
    "# !pip install torch numpy pandas scikit-learn shap captum matplotlib tqdm\n",
    "\n",
    "import os, re, glob, math, random, json, warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_absolute_error, balanced_accuracy_score\n",
    "\n",
    "# XAI\n",
    "import shap\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e216bf",
   "metadata": {},
   "source": [
    "## 1) Configure dataset paths (Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "181e20be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAIT_ROOT = C:\\Users\\muham\\_Projects\\PD1\\Dataset\\gait-in-parkinsons-disease-1.0.0\n",
      "VOICE_TELEMONITORING_CSV = C:\\Users\\muham\\_Projects\\PD1\\Dataset\\parkinsons_updrs.data\n",
      "PADS_ROOT = C:\\Users\\muham\\_Projects\\PD1\\Dataset\\pads-parkinsons-disease-smartwatch-dataset-1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Edit these if your folders are different\n",
    "GAIT_ROOT = r\"C:\\Users\\muham\\_Projects\\PD1\\Dataset\\gait-in-parkinsons-disease-1.0.0\"\n",
    "VOICE_TELEMONITORING_CSV = r\"C:\\Users\\muham\\_Projects\\PD1\\Dataset\\parkinsons_updrs.data\"\n",
    "PADS_ROOT = r\"C:\\Users\\muham\\_Projects\\PD1\\Dataset\\pads-parkinsons-disease-smartwatch-dataset-1.0.0\"\n",
    "\n",
    "print(\"GAIT_ROOT =\", GAIT_ROOT)\n",
    "print(\"VOICE_TELEMONITORING_CSV =\", VOICE_TELEMONITORING_CSV)\n",
    "print(\"PADS_ROOT =\", PADS_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894bd3d3",
   "metadata": {},
   "source": [
    "## 2) Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f00b89b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(s)\n",
    "\n",
    "def subject_partition_indices(subject_ids: List[str]):\n",
    "    mp: Dict[str, List[int]] = {}\n",
    "    for i,s in enumerate(subject_ids):\n",
    "        mp.setdefault(s, []).append(i)\n",
    "    return mp\n",
    "\n",
    "def make_client_loaders(idx_list: List[int], ds, bs=64, shuffle=True, drop_last=True):\n",
    "    class _Sub(Dataset):\n",
    "        def __init__(self, base, idx): self.base=base; self.idx=idx\n",
    "        def __len__(self): return len(self.idx)\n",
    "        def __getitem__(self, i): return self.base[self.idx[i]]\n",
    "    return DataLoader(_Sub(ds, idx_list), batch_size=bs, shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "def clone_sd(sd): return {k:v.clone() for k,v in sd.items()}\n",
    "def avg_sd(sd_list: List[dict], weights: List[float]):\n",
    "    out={}\n",
    "    tot = sum(weights)\n",
    "    for k in sd_list[0].keys():\n",
    "        out[k] = sum(w*sd[k] for sd,w in zip(sd_list,weights))/tot\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f7e59",
   "metadata": {},
   "source": [
    "## 3) Gait dataset loader + model (BiLSTM for PD vs Control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "263d5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaitDataset(Dataset):\n",
    "    def __init__(self, X, y, subj):\n",
    "        self._X = X; self._y = y; self._s = subj\n",
    "    def __len__(self): return len(self._X)\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self._X[i], dtype=torch.float32), torch.tensor(self._y[i], dtype=torch.long)\n",
    "    @property\n",
    "    def subjects(self): return self._s\n",
    "\n",
    "def infer_label_from_filename(fn_lower: str):\n",
    "    # Robust markers for PD vs Control\n",
    "    pd_markers = [\"gapt\", \"jupt\", \"sipt\", \"_pt\", \"-pt\", \"parkinson\", \"pd\"]\n",
    "    co_markers = [\"gaco\", \"juco\", \"sico\", \"_co\", \"-co\", \"control\", \"healthy\"]\n",
    "    if any(m in fn_lower for m in pd_markers): return 1\n",
    "    if any(m in fn_lower for m in co_markers): return 0\n",
    "    return None\n",
    "\n",
    "def subject_id_from_name(path: str):\n",
    "    stem = os.path.splitext(os.path.basename(path))[0]\n",
    "    m = re.match(r\"([A-Za-z]+[A-Za-z]*\\d+)\", stem)\n",
    "    return m.group(1) if m else stem\n",
    "\n",
    "def should_skip_file(path: str):\n",
    "    # Skip metadata/text/hash files\n",
    "    basename = os.path.basename(path).lower()\n",
    "    return any(basename.startswith(k) for k in [\"format\", \"demographics\", \"sha256sums\", \"notes\", \"readme\"])\n",
    "\n",
    "def load_gait_windows(root: str, window_sec=4.0, step_sec=2.0, fs=100.0):\n",
    "    files = glob.glob(os.path.join(root, \"*.txt\"))\n",
    "    X, y, sids = [], [], []\n",
    "    win = int(window_sec*fs); step=int(step_sec*fs)\n",
    "    for f in tqdm(files, desc=\"GAIT: loading\"):\n",
    "        try:\n",
    "            if should_skip_file(f):\n",
    "                continue\n",
    "            arr = np.loadtxt(f)\n",
    "            if arr.ndim!=2 or arr.shape[1] < 19:\n",
    "                continue\n",
    "            label = infer_label_from_filename(f.lower())\n",
    "            if label is None:\n",
    "                continue\n",
    "            sid = subject_id_from_name(f)\n",
    "            T = arr.shape[0]\n",
    "            for start in range(0, max(1, T - win + 1), step):\n",
    "                seg = arr[start:start+win, :19]\n",
    "                if seg.shape[0] < win: break\n",
    "                X.append(seg); y.append(label); sids.append(sid)\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] {f}: {e}\")\n",
    "    return X, y, sids\n",
    "\n",
    "class GaitBiLSTM(nn.Module):\n",
    "    def __init__(self, in_dim=19, hidden=64, layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(in_dim, hidden, num_layers=layers, batch_first=True, bidirectional=True)\n",
    "        self.head = nn.Sequential(nn.Linear(2*hidden, 64), nn.ReLU(), nn.Linear(64, 2))\n",
    "    def forward(self, x):  # (B,T,19)\n",
    "        out,_ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        return self.head(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328405a5",
   "metadata": {},
   "source": [
    "### 3.1) Train Federated Gait Classifier (FedAvg) — with robust evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b8e2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GAIT: loading: 100%|█████████████████████████████████████████████████████████████████| 309/309 [00:11<00:00, 26.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAIT samples=16117  subjects=165\n",
      "\n",
      "=== GAIT Round 1/5 ===\n",
      "[Global] ACC=0.838  F1=0.912  AUC=nan  BACC=0.838\n",
      "\n",
      "=== GAIT Round 2/5 ===\n",
      "[Global] ACC=1.000  F1=1.000  AUC=nan  BACC=1.000\n",
      "\n",
      "=== GAIT Round 3/5 ===\n",
      "[Global] ACC=1.000  F1=1.000  AUC=nan  BACC=1.000\n",
      "\n",
      "=== GAIT Round 4/5 ===\n",
      "[Global] ACC=1.000  F1=1.000  AUC=nan  BACC=1.000\n",
      "\n",
      "=== GAIT Round 5/5 ===\n",
      "[Global] ACC=1.000  F1=1.000  AUC=nan  BACC=1.000\n"
     ]
    }
   ],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "# Load gait data\n",
    "Xg, yg, subjg = load_gait_windows(GAIT_ROOT, window_sec=4.0, step_sec=2.0, fs=100.0)\n",
    "gait_ds = GaitDataset(Xg, yg, subjg)\n",
    "parts_g = subject_partition_indices(gait_ds.subjects)\n",
    "print(f\"GAIT samples={len(gait_ds)}  subjects={len(parts_g)}\")\n",
    "\n",
    "# Init global model\n",
    "model_g = GaitBiLSTM(in_dim=19, hidden=64).to(DEVICE)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "rounds = 5\n",
    "frac_clients = 0.3\n",
    "local_epochs = 2\n",
    "bs = 128\n",
    "\n",
    "def train_epoch_cls(model, dl, opt, crit):\n",
    "    model.train(); total=0; correct=0\n",
    "    for xb, yb in dl:\n",
    "        xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
    "        opt.zero_grad(); logits = model(xb); loss = crit(logits, yb)\n",
    "        loss.backward(); opt.step()\n",
    "        pred = logits.argmax(1); total += yb.size(0); correct += (pred==yb).sum().item()\n",
    "    return correct/max(1,total)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_cls(model, dl):\n",
    "    model.eval()\n",
    "    y_true, y_score = [], []\n",
    "    for xb,yb in dl:\n",
    "        xb=xb.to(DEVICE)\n",
    "        prob = torch.softmax(model(xb), dim=1)[:,1].cpu().numpy()\n",
    "        y_true.extend(yb.numpy().tolist()); y_score.extend(prob.tolist())\n",
    "    y_pred = [1 if s>=0.5 else 0 for s in y_score]\n",
    "    acc = accuracy_score(y_true,y_pred)\n",
    "    f1  = f1_score(y_true,y_pred)\n",
    "    bacc= balanced_accuracy_score(y_true,y_pred)\n",
    "    # AUC only when both classes present\n",
    "    auc = roc_auc_score(y_true,y_score) if len(set(y_true))==2 else float(\"nan\")\n",
    "    return acc, f1, auc, bacc\n",
    "\n",
    "for r in range(rounds):\n",
    "    print(f\"\\n=== GAIT Round {r+1}/{rounds} ===\")\n",
    "    client_ids = list(parts_g.keys())\n",
    "    m = max(1, int(frac_clients*len(client_ids)))\n",
    "    chosen = random.sample(client_ids, m)\n",
    "\n",
    "    sds=[]; ws=[]\n",
    "    for cid in chosen:\n",
    "        idx = parts_g[cid]\n",
    "        if len(idx) < bs:  # need enough for a batch\n",
    "            continue\n",
    "        local = GaitBiLSTM(in_dim=19, hidden=64).to(DEVICE)\n",
    "        local.load_state_dict(clone_sd(model_g.state_dict()))\n",
    "        dl = make_client_loaders(idx, gait_ds, bs=bs, shuffle=True, drop_last=True)\n",
    "        opt = torch.optim.Adam(local.parameters(), lr=1e-3)\n",
    "        for _ in range(local_epochs):\n",
    "            train_epoch_cls(local, dl, opt, crit)\n",
    "        sds.append(clone_sd(local.state_dict())); ws.append(len(idx))\n",
    "\n",
    "    if sds:\n",
    "        model_g.load_state_dict(avg_sd(sds, ws))\n",
    "\n",
    "    # Evaluate on full dataset (no drop_last, no shuffle)\n",
    "    dl_eval = DataLoader(gait_ds, batch_size=bs, shuffle=False, drop_last=False)\n",
    "    acc,f1,auc,bacc = eval_cls(model_g, dl_eval)\n",
    "    print(f\"[Global] ACC={acc:.3f}  F1={f1:.3f}  AUC={auc:.3f}  BACC={bacc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfc3e7f",
   "metadata": {},
   "source": [
    "### 3.2) XAI on Gait (Integrated Gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4eda26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 400, 19)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute IG on a mini-batch\n",
    "model_g.eval()\n",
    "xb,_ = next(iter(DataLoader(gait_ds, batch_size=32, shuffle=True, drop_last=True)))\n",
    "xb = xb.to(DEVICE)\n",
    "ig = IntegratedGradients(lambda inp: torch.softmax(model_g(inp), dim=1)[:,1])\n",
    "attr = ig.attribute(xb, n_steps=32).detach().cpu().numpy()\n",
    "attr.shape  # (B, T, 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1e31e",
   "metadata": {},
   "source": [
    "## 4) Voice Telemonitoring UPDRS Regression (MLP + FedAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e2a7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceTeleDataset(Dataset):\n",
    "    def __init__(self, X, y, subj):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1,1)\n",
    "        self.subj = subj\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self,i): return self.X[i], self.y[i]\n",
    "    @property\n",
    "    def subjects(self): return self.subj\n",
    "\n",
    "def load_voice_telemonitoring(csv_path: str, target=\"total_UPDRS\"):\n",
    "    cols = [\"subject\",\"age\",\"sex\",\"test_time\",\"motor_UPDRS\",\"total_UPDRS\",\n",
    "            \"Jitter(%)\",\"Jitter(Abs)\",\"Jitter:RAP\",\"Jitter:PPQ5\",\"Jitter:DDP\",\n",
    "            \"Shimmer\",\"Shimmer(dB)\",\"Shimmer:APQ3\",\"Shimmer:APQ5\",\"Shimmer:APQ11\",\"Shimmer:DDA\",\n",
    "            \"NHR\",\"HNR\",\"RPDE\",\"DFA\",\"PPE\"]\n",
    "    df = pd.read_csv(csv_path, header=None, names=cols)\n",
    "    y = df[target].values.astype(np.float32)\n",
    "    X = df.drop(columns=[\"subject\", target]).values.astype(np.float32)\n",
    "    subj = df[\"subject\"].astype(str).tolist()\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Xs = scaler.transform(X)\n",
    "    return Xs, y, subj, scaler, [c for c in df.columns if c not in [\"subject\", target]]\n",
    "\n",
    "class VoiceMLPReg(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0655f347",
   "metadata": {},
   "source": [
    "### 4.1) Train Federated Voice Regressor (FedAvg on UPDRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8feba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "Xv, yv, sub_v, scaler_v, feat_names = load_voice_telemonitoring(VOICE_TELEMONITORING_CSV, target=\"total_UPDRS\")\n",
    "voice_ds = VoiceTeleDataset(Xv, yv, sub_v)\n",
    "parts_v = subject_partition_indices(voice_ds.subjects)\n",
    "print(f\"VOICE samples={len(voice_ds)}  subjects={len(parts_v)}\")\n",
    "\n",
    "in_dim = voice_ds[0][0].numel()\n",
    "model_v = VoiceMLPReg(in_dim).to(DEVICE)\n",
    "\n",
    "rounds = 5\n",
    "frac_clients = 0.3\n",
    "local_epochs = 2\n",
    "bs = 256\n",
    "\n",
    "def train_epoch_reg(model, dl, opt):\n",
    "    model.train(); losses=[]\n",
    "    for xb,yb in dl:\n",
    "        xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
    "        opt.zero_grad(); pred = model(xb)\n",
    "        loss = nn.functional.l1_loss(pred, yb)\n",
    "        loss.backward(); opt.step(); losses.append(loss.item())\n",
    "    return float(np.mean(losses)) if losses else 0.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_reg(model, dl):\n",
    "    model.eval(); preds=[]; trues=[]\n",
    "    for xb,yb in dl:\n",
    "        xb=xb.to(DEVICE)\n",
    "        pred = model(xb).cpu().numpy().ravel()\n",
    "        preds.extend(pred.tolist()); trues.extend(yb.numpy().ravel().tolist())\n",
    "    return mean_absolute_error(trues, preds)\n",
    "\n",
    "for r in range(rounds):\n",
    "    print(f\"\\n=== VOICE Round {r+1}/{rounds} ===\")\n",
    "    client_ids = list(parts_v.keys())\n",
    "    m = max(1, int(frac_clients*len(client_ids)))\n",
    "    chosen = random.sample(client_ids, m)\n",
    "\n",
    "    sds=[]; ws=[]\n",
    "    for cid in chosen:\n",
    "        idx = parts_v[cid]\n",
    "        if len(idx) < bs: continue\n",
    "        local = VoiceMLPReg(in_dim).to(DEVICE)\n",
    "        local.load_state_dict(clone_sd(model_v.state_dict()))\n",
    "        dl = make_client_loaders(idx, voice_ds, bs=bs, shuffle=True, drop_last=True)\n",
    "        opt = torch.optim.Adam(local.parameters(), lr=1e-3)\n",
    "        for _ in range(local_epochs):\n",
    "            mae = train_epoch_reg(local, dl, opt)\n",
    "        sds.append(clone_sd(local.state_dict())); ws.append(len(idx))\n",
    "\n",
    "    if sds:\n",
    "        model_v.load_state_dict(avg_sd(sds, ws))\n",
    "\n",
    "    dl_eval = DataLoader(voice_ds, batch_size=bs, shuffle=False, drop_last=False)\n",
    "    mae = eval_reg(model_v, dl_eval)\n",
    "    print(f\"[Global] total_UPDRS MAE={mae:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e71365",
   "metadata": {},
   "source": [
    "### 4.2) XAI on Voice (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc15eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP on a small sample\n",
    "model_v.eval()\n",
    "small = Xv[:50]\n",
    "\n",
    "def predict_fn(inp):\n",
    "    with torch.no_grad():\n",
    "        inp_t = torch.tensor(inp, dtype=torch.float32, device=DEVICE)\n",
    "        return model_v(inp_t).cpu().numpy().ravel()\n",
    "\n",
    "explainer = shap.KernelExplainer(predict_fn, small[:10])\n",
    "sv = explainer.shap_values(small[10:20], nsamples=100)\n",
    "print(\"SHAP computed. Visualize locally with:\")\n",
    "print(\"shap.summary_plot(sv, features=small[10:20], feature_names=feat_names)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3121c242",
   "metadata": {},
   "source": [
    "## 5) (Optional) PADS parser scaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6220e228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PADS] Found patient patient_001.json keys: ['resource_type', 'id', 'study_id', 'condition', 'disease_comment', 'age_at_diagnosis', 'age', 'height'] ...\n",
      "[PADS] Found patient patient_002.json keys: ['resource_type', 'id', 'study_id', 'condition', 'disease_comment', 'age_at_diagnosis', 'age', 'height'] ...\n",
      "[PADS] Found patient patient_003.json keys: ['resource_type', 'id', 'study_id', 'condition', 'disease_comment', 'age_at_diagnosis', 'age', 'height'] ...\n"
     ]
    }
   ],
   "source": [
    "def parse_pads_example(pads_root=PADS_ROOT):\n",
    "    patients = glob.glob(os.path.join(pads_root, \"patients\", \"patient_*.json\"))\n",
    "    for pj in patients[:3]:\n",
    "        with open(pj, \"r\", encoding=\"utf-8\") as f:\n",
    "            meta = json.load(f)\n",
    "        pid = meta.get(\"patient_id\", os.path.basename(pj))\n",
    "        print(f\"[PADS] Found patient {pid} keys: {list(meta.keys())[:8]} ...\")\n",
    "\n",
    "parse_pads_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af0546-8660-404a-87c5-16bcd5947a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ad7b6-6d03-47a3-8908-9350a962a3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Parkinsons Env)",
   "language": "python",
   "name": "parkinsons_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
